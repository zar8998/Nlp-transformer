# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cIjeF8U46MSxlhY8m25IIsubqZBz7ExB
"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

# -------------------------------
# داده اولیه
# -------------------------------
texts = [
    "سلام دوست من، امیدوارم روز خوبی داشته باشی",
    "یادگیری برنامه‌نویسی بسیار لذت‌بخش است",
    "امروز می‌خواهم یک مدل ساده پیش‌بینی کلمه بعدی بسازم",
    "شب خوب است برای مطالعه و یادگیری"
]

# --------------------------------------
# 2. تبدیل کلمات به اعداد (توکنایزر)
# --------------------------------------
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# ----------------------------------------
# 3. ساخت داده‌های ورودی و خروجی برای مدل
# هر کلمه به عنوان ورودی و کلمه بعدی به عنوان خروجی
# ----------------------------------------
input_sequences = []
output_words = []

for sequence in sequences:
    for i in range(len(sequence) - 1):
        input_sequences.append(sequence[i])
        output_words.append(sequence[i + 1])

# -----------------------------------
# 4. آماده‌سازی داده‌ها برای مدل
# تبدیل ورودی به آرایه numpy با ابعاد (تعداد نمونه‌ها, طول دنباله=1)
# خروجی به صورت دسته‌ای (one-hot encoding)
# -----------------------------------
vocab_size = len(tokenizer.word_index) + 1  # +1 برای کلمه padding احتمالی

X = np.array(input_sequences)
X = X.reshape((X.shape[0], 1, 1))  # اصلاح: ورودی 3 بعدی (batch_size, timesteps=1, features=1)
y = to_categorical(output_words, num_classes=vocab_size)

# ---------------------------------
# 5. ساخت مدل RNN ساده با Keras
# ---------------------------------
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=8, input_shape=(1,)),
    SimpleRNN(16),
    Dense(vocab_size, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()  # نمایش ساختار مدل برای دیباگ

# -------------------------
# 6. آموزش مدل
# -------------------------
model.fit(X, y, epochs=100, batch_size=16, verbose=1)  # کاهش epochs و تنظیم batch_size
print("Training complete!")

# ------------------------------------
# 7. تابع پیش‌بینی کلمه بعدی
# ورودی: مدل، توکنایزر، و متن ورودی (کلمه یا جمله)
# خروجی: کلمه بعدی پیش‌بینی شده
# ------------------------------------
def predict_next_word(model, tokenizer, text):
    seq = tokenizer.texts_to_sequences([text])
    if not seq[0]:  # بررسی خالی بودن دنباله
        return "کلمه نامشخص"
    last_word = np.array([seq[0][-1]])
    last_word = last_word.reshape((1, 1, 1))  # اصلاح: ورودی 3 بعدی برای پیش‌بینی
    pred = model.predict(last_word, verbose=0)
    pred_word_index = np.argmax(pred, axis=-1)[0]
    for word, index in tokenizer.word_index.items():
        if index == pred_word_index:
            return word
    return "کلمه نامشخص"

# -----------------------
# 8. تست مدل با کلمات مختلف
# -----------------------
test_words = ["سلام", "دوست", "یادگیری", "امروز", "شب"]
for w in test_words:
    print(f"Next word after '{w}': {predict_next_word(model, tokenizer, w)}")

