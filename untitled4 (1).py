# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XpUOkiIMDbRehfNIPYfzGFaUZEcQCn98
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, MultiHeadAttention, Dropout
import matplotlib.pyplot as plt

# Clear previous session to avoid memory issues
tf.keras.backend.clear_session()

# -------------------------------
# 1. تعریف لایه Transformer
# -------------------------------
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim)
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=False):  # پیش‌فرض برای training
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# -------------------------------
# 2. داده اولیه (متن‌های فارسی)
# -------------------------------
texts = [
    "سلام دوست من، امیدوارم روز خوبی داشته باشی",
    "یادگیری برنامه‌نویسی بسیار لذت‌بخش است",
    "امروز می‌خواهم یک مدل ساده پیش‌بینی کلمه بعدی بسازم",
    "شب خوب است برای مطالعه و یادگیری"
]

# --------------------------------------
# 3. تبدیل کلمات به اعداد (توکنایزر)
# --------------------------------------
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# ----------------------------------------
# 4. ساخت داده‌های ورودی و خروجی
# ----------------------------------------
input_sequences = []
output_words = []

for sequence in sequences:
    for i in range(len(sequence) - 1):
        input_sequences.append(sequence[i])
        output_words.append(sequence[i + 1])

# -----------------------------------
# 5. آماده‌سازی داده‌ها برای مدل
# -----------------------------------
vocab_size = len(tokenizer.word_index) + 1
embed_dim = 32
num_heads = 2
ff_dim = 32

X = np.array(input_sequences)
X = X.reshape((X.shape[0], 1))  # ورودی 2 بعدی: (batch_size, timesteps=1)
y = to_categorical(output_words, num_classes=vocab_size)

# ---------------------------------
# 6. ساخت مدل Transformer
# ---------------------------------
inputs = Input(shape=(1,))  # اصلاح شکل ورودی
embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)
transformer_block = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(embedding)
flat = tf.keras.layers.Flatten()(transformer_block)
outputs = Dense(vocab_size, activation='softmax')(flat)

model = Model(inputs=inputs, outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# -------------------------
# 7. آموزش مدل
# -------------------------
history = model.fit(X, y, epochs=50, batch_size=8, verbose=1, validation_split=0.2)

# ذخیره مدل
model.save('/content/transformer_model.h5')

# -------------------------
# 8. رسم نمودار دقت و خطا
# -------------------------
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('/content/training_plots.png')
plt.show()

# ------------------------------------
# 9. تابع پیش‌بینی کلمه بعدی
# ------------------------------------
def predict_next_word(model, tokenizer, text):
    seq = tokenizer.texts_to_sequences([text])
    if not seq[0]:
        return "کلمه نامشخص"
    last_word = np.array([seq[0][-1]])
    last_word = last_word.reshape((1, 1))  # اصلاح شکل برای پیش‌بینی
    pred = model.predict(last_word, verbose=0)
    pred_word_index = np.argmax(pred, axis=-1)[0]
    for word, index in tokenizer.word_index.items():
        if index == pred_word_index:
            return word
    return "کلمه نامشخص"

# -----------------------
# 10. تست مدل
# -----------------------
test_words = ["سلام", "دوست", "یادگیری", "امروز", "شب"]
for w in test_words:
    print(f"Next word after '{w}': {predict_next_word(model, tokenizer, w)}")

